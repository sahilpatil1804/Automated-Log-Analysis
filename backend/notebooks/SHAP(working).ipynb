{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbY-eTHAIvv",
        "outputId": "a76bd9cb-81df-4266-c65b-14b1ea77096b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[⚙️] Processing /content/2good_reqff.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✅] Saved shap_explanations_goodbad.csv\n",
            "[⚙️] Processing /content/wls_day-02.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✅] Saved shap_explanations_network.csv\n",
            "[⚙️] Processing /content/netflow_day-02.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✅] Saved shap_explanations_host.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import shap\n",
        "\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def load_and_preprocess(csv_path, feature_cols, max_rows=5000):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if len(df) > max_rows:\n",
        "        df = df.sample(n=max_rows, random_state=42)\n",
        "\n",
        "    df = df.dropna(subset=feature_cols)\n",
        "\n",
        "    # Convert hex strings to integers\n",
        "    for col in feature_cols:\n",
        "        df[col] = df[col].apply(hex_to_int)\n",
        "\n",
        "    df = df.dropna(subset=feature_cols)\n",
        "    df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "\n",
        "    return df, df[feature_cols].values\n",
        "\n",
        "def train_autoencoder(X_train):\n",
        "    model = Sequential([\n",
        "        Dense(2, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(X_train.shape[1], activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    model.fit(X_train, X_train, epochs=20, batch_size=32, verbose=0)\n",
        "    return model\n",
        "\n",
        "def compute_anomaly_scores(autoencoder, X):\n",
        "    recon = autoencoder.predict(X, verbose=0)\n",
        "    return np.mean(np.abs(X - recon), axis=1)\n",
        "\n",
        "def explain_with_shap(model, X, feature_names, scores, top_n=200):\n",
        "    top_idx = np.argsort(scores)[-top_n:]\n",
        "    X_top = X[top_idx]\n",
        "    scores_top = scores[top_idx]\n",
        "\n",
        "    explainer = shap.TreeExplainer(model, data=X_top, feature_perturbation=\"interventional\")\n",
        "    shap_values = explainer.shap_values(X_top, approximate=True)\n",
        "\n",
        "    explanations = []\n",
        "    for i, row in enumerate(shap_values):\n",
        "        # Get top 2 contributing features\n",
        "        top_features_idx = np.argsort(np.abs(row))[-2:]\n",
        "        top_features = []\n",
        "        for idx in reversed(top_features_idx):\n",
        "            feature = feature_names[idx]\n",
        "            direction = \"high\" if row[idx] > 0 else \"low\"\n",
        "            top_features.append(f\"{direction} {feature}\")\n",
        "\n",
        "        explanation = f\"Log flagged due to {top_features[0]} and {top_features[1]} contributing to anomaly score.\"\n",
        "        explanations.append(explanation)\n",
        "\n",
        "    # Build final DataFrame\n",
        "    explanation_df = pd.DataFrame({\n",
        "        \"explanation\": explanations,\n",
        "        \"anomaly_score\": scores_top\n",
        "    })\n",
        "\n",
        "    return explanation_df, top_idx\n",
        "\n",
        "# === CONFIG ===\n",
        "csv_list = [\n",
        "    (\"/content/2good_reqff.csv\", [\"path_length\", \"body_length\", \"badwords_count\"], \"shap_explanations_goodbad.csv\"),\n",
        "    (\"/content/wls_day-02.csv\", [\"ProcessID\", \"ParentProcessID\", \"EventID\"], \"shap_explanations_network.csv\"),\n",
        "    (\"/content/netflow_day-02.csv\", [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"], \"shap_explanations_host.csv\")\n",
        "]\n",
        "\n",
        "# === PIPELINE ===\n",
        "for path, features, out_csv in csv_list:\n",
        "    print(f\"[⚙️] Processing {path}...\")\n",
        "    df, X = load_and_preprocess(path, features)\n",
        "    scaler = StandardScaler().fit(X)\n",
        "    X_scaled = scaler.transform(X)\n",
        "\n",
        "    ae = train_autoencoder(X_scaled)\n",
        "    scores = compute_anomaly_scores(ae, X_scaled)\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_scaled, scores)\n",
        "    shap_df, top_idx = explain_with_shap(rf, X_scaled, features, scores)\n",
        "\n",
        "    final_df = pd.concat([df.iloc[top_idx].reset_index(drop=True), shap_df.add_prefix(\"shap_\")], axis=1)\n",
        "    final_df.to_csv(out_csv, index=False)\n",
        "    print(f\"[✅] Saved {out_csv}\")\n"
      ]
    }
  ]
}