{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC0fI7wLbgN1",
        "outputId": "eb4a2262-9351-49f7-9492-a18ce028b091"
      },
      "outputs": [],
      "source": [
        "# Required libraries: pandas, scikit-learn, tensorflow\n",
        "# You can install them using pip:\n",
        "# pip install pandas scikit-learn tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# --- Part 1: Generate Scores from the Autoencoder ---\n",
        "try:\n",
        "    df = pd.read_csv('../datasets/cybersecurity_attacks.csv')\n",
        "    print(\"üìÑ Successfully loaded original 'cybersecurity_attacks.csv'.\")\n",
        "    df = df.sample(n=10000, random_state=42).copy()\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'cybersecurity_attacks.csv' not found.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Define features for the autoencoder\n",
        "    numerical_features = ['Packet Length', 'Source Port', 'Destination Port']\n",
        "    categorical_features = ['Protocol', 'Packet Type', 'Traffic Type', 'Attack Type', 'Action Taken', 'Severity Level']\n",
        "\n",
        "    df_model = df.dropna(subset=numerical_features + categorical_features).copy()\n",
        "\n",
        "    # Preprocessing\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', MinMaxScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ])\n",
        "    X_processed = preprocessor.fit_transform(df_model)\n",
        "    if hasattr(X_processed, \"toarray\"):\n",
        "        X_processed = X_processed.toarray()\n",
        "\n",
        "    # Build and train the Autoencoder\n",
        "    print(\"ü§ñ Training autoencoder to generate new scores...\")\n",
        "    input_dim = X_processed.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(64, activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(32, activation=\"relu\")(encoder)\n",
        "    encoder = Dense(14, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(32, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(64, activation=\"relu\")(decoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_processed, X_processed, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"‚úÖ Autoencoder training complete.\")\n",
        "\n",
        "    predictions = autoencoder.predict(X_processed)\n",
        "    df_model['Autoencoder_Anomaly_Score'] = np.mean(np.abs(X_processed - predictions), axis=1)\n",
        "\n",
        "    # --- Part 2: Combine the Scores ---\n",
        "    print(\"\\nüîÑ Normalizing and combining scores...\")\n",
        "    df = df.merge(df_model[['Autoencoder_Anomaly_Score']], left_index=True, right_index=True, how='left')\n",
        "    df.dropna(subset=['Anomaly Scores', 'Autoencoder_Anomaly_Score'], inplace=True)\n",
        "    scaler = MinMaxScaler()\n",
        "    df['Original_Scaled_Score'] = scaler.fit_transform(df[['Anomaly Scores']])\n",
        "    df['Autoencoder_Scaled_Score'] = scaler.fit_transform(df[['Autoencoder_Anomaly_Score']])\n",
        "    df['Combined_Score'] = (df['Original_Scaled_Score'] + df['Autoencoder_Scaled_Score']) / 2\n",
        "    print(\"‚úÖ Combined_Score created.\")\n",
        "\n",
        "    # --- Part 3: Classify Logs Based on a Threshold ---\n",
        "    print(\"\\nüîé Setting a threshold to classify threats...\")\n",
        "\n",
        "    # Define what percentage of data to classify as a threat (e.g., top 5%)\n",
        "    # You can tune this value (e.g., 0.01 for top 1%, 0.10 for top 10%)\n",
        "    anomaly_percentage = 0.05\n",
        "\n",
        "    # Calculate the threshold based on the percentile\n",
        "    threshold = df['Combined_Score'].quantile(1 - anomaly_percentage)\n",
        "\n",
        "    print(f\"Threshold set at the {100 * (1 - anomaly_percentage):.0f}th percentile: {threshold:.4f}\")\n",
        "\n",
        "    # Classify based on the threshold\n",
        "    df['Classification'] = df['Combined_Score'].apply(lambda x: 'Threat' if x > threshold else 'Not a Threat')\n",
        "\n",
        "    # --- Display Final Results ---\n",
        "    print(\"\\n--- Classification Results ---\")\n",
        "    print(df['Classification'].value_counts())\n",
        "\n",
        "    print(\"\\n--- Top 5 Classified Threats ---\")\n",
        "    display_cols = ['Attack Type', 'Combined_Score', 'Classification']\n",
        "    threats_df = df[df['Classification'] == 'Threat'].sort_values(by='Combined_Score', ascending=False)\n",
        "    print(threats_df[display_cols].head().to_string())\n",
        "\n",
        "    # Save the final results to a new file\n",
        "    df.to_csv('cybersecurity_attacks_with_classifications.csv', index=False)\n",
        "    print(\"\\nüíæ Results saved to 'cybersecurity_attacks_with_classifications.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVQK5_J2h4_v",
        "outputId": "34254016-9834-4cdb-a3a0-e5a09c8fe95c"
      },
      "outputs": [],
      "source": [
        "# Required libraries: pandas, scikit-learn, tensorflow\n",
        "# You can install them using pip:\n",
        "# pip install pandas scikit-learn tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# --- Part 1: Generate Scores from the Autoencoder ---\n",
        "try:\n",
        "    df = pd.read_csv('cybersecurity_attacks.csv')\n",
        "    print(\"üìÑ Successfully loaded original 'cybersecurity_attacks.csv'.\")\n",
        "    df = df.sample(n=10000, random_state=42).copy()\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'cybersecurity_attacks.csv' not found.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Define features for the autoencoder\n",
        "    numerical_features = ['Packet Length', 'Source Port', 'Destination Port']\n",
        "    categorical_features = ['Protocol', 'Packet Type', 'Traffic Type', 'Attack Type', 'Action Taken', 'Severity Level']\n",
        "\n",
        "    df_model = df.dropna(subset=numerical_features + categorical_features).copy()\n",
        "\n",
        "    # Preprocessing\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', MinMaxScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ])\n",
        "    X_processed = preprocessor.fit_transform(df_model)\n",
        "    if hasattr(X_processed, \"toarray\"):\n",
        "        X_processed = X_processed.toarray()\n",
        "\n",
        "    # Build and train the Autoencoder\n",
        "    print(\"ü§ñ Training autoencoder to generate new scores...\")\n",
        "    input_dim = X_processed.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(64, activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(32, activation=\"relu\")(encoder)\n",
        "    encoder = Dense(14, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(32, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(64, activation=\"relu\")(decoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_processed, X_processed, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"‚úÖ Autoencoder training complete.\")\n",
        "\n",
        "    predictions = autoencoder.predict(X_processed)\n",
        "    df_model['Autoencoder_Anomaly_Score'] = np.mean(np.abs(X_processed - predictions), axis=1)\n",
        "\n",
        "    # --- Part 2: Combine the Scores ---\n",
        "    print(\"\\nüîÑ Normalizing and combining scores...\")\n",
        "    df = df.merge(df_model[['Autoencoder_Anomaly_Score']], left_index=True, right_index=True, how='left')\n",
        "    df.dropna(subset=['Anomaly Scores', 'Autoencoder_Anomaly_Score'], inplace=True)\n",
        "    scaler = MinMaxScaler()\n",
        "    df['Original_Scaled_Score'] = scaler.fit_transform(df[['Anomaly Scores']])\n",
        "    df['Autoencoder_Scaled_Score'] = scaler.fit_transform(df[['Autoencoder_Anomaly_Score']])\n",
        "    df['Combined_Score'] = (df['Original_Scaled_Score'] + df['Autoencoder_Scaled_Score']) / 2\n",
        "    print(\"‚úÖ Combined_Score created.\")\n",
        "\n",
        "    # --- Part 3: Classify Threats into Levels Based on Quantiles ---\n",
        "    print(\"\\nüîé Classifying threats into levels based on score quantiles...\")\n",
        "\n",
        "    # Define the percentile cutoffs for each threat level\n",
        "    level_3_threshold = df['Combined_Score'].quantile(0.99) # Top 1%\n",
        "    level_2_threshold = df['Combined_Score'].quantile(0.95) # Top 5%\n",
        "    level_1_threshold = df['Combined_Score'].quantile(0.90) # Top 10%\n",
        "\n",
        "    print(f\"Level 3 Threat threshold (Top 1%): > {level_3_threshold:.4f}\")\n",
        "    print(f\"Level 2 Threat threshold (Top 5%): > {level_2_threshold:.4f}\")\n",
        "    print(f\"Level 1 Threat threshold (Top 10%): > {level_1_threshold:.4f}\")\n",
        "\n",
        "    # Create a list of conditions and corresponding level assignments\n",
        "    conditions = [\n",
        "        df['Combined_Score'] > level_3_threshold,\n",
        "        df['Combined_Score'] > level_2_threshold,\n",
        "        df['Combined_Score'] > level_1_threshold\n",
        "    ]\n",
        "    levels = ['Level 3 Threat (Critical)', 'Level 2 Threat (High)', 'Level 1 Threat (Medium)']\n",
        "\n",
        "    # Use numpy.select for efficient conditional assignment\n",
        "    df['Threat_Level'] = np.select(conditions, levels, default='Not a Threat')\n",
        "\n",
        "    # --- Display Final Results ---\n",
        "    print(\"\\n--- Threat Level Distribution ---\")\n",
        "    print(df['Threat_Level'].value_counts())\n",
        "\n",
        "    print(\"\\n--- Sample of Highest-Level Threats ---\")\n",
        "    display_cols = ['Attack Type', 'Combined_Score', 'Threat_Level']\n",
        "    # Show the top threats from any level, sorted by score\n",
        "    threats_df = df[df['Threat_Level'] != 'Not a Threat'].sort_values(by='Combined_Score', ascending=False)\n",
        "    print(threats_df[display_cols].head().to_string())\n",
        "\n",
        "    # Save the final results to a new file\n",
        "    df.to_csv('cybersecurity_attacks_with_threat_levels.csv', index=False)\n",
        "    print(\"\\nüíæ Results saved to 'cybersecurity_attacks_with_threat_levels.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_AfKPa3pcKa",
        "outputId": "7c39dc15-a69c-43c2-853f-4b2cc111e909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Hybrid Anomaly Classification Pipeline\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 1/4: ../datasets/cybersecurity_attacks.csv ---\n",
            "  üìÑ Loaded 40000 rows from ../datasets/cybersecurity_attacks.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_cybersecurity_attacks.csv\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 2/4: ../datasets/2good_reqff.csv ---\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step\n",
            "  ‚úÖ Generated scores using supervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 5219\n",
            "Level 1 Threat (Medium)       288\n",
            "Level 2 Threat (High)         232\n",
            "Level 3 Threat (Critical)      58\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_goodbad_requests.csv\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 3/4: ../datasets/wls_day-02.csv ---\n",
            "  üìÑ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  ‚úÖ Final unsupervised dataset: 2604 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 2343\n",
            "Level 1 Threat (Medium)       130\n",
            "Level 2 Threat (High)         104\n",
            "Level 3 Threat (Critical)      27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_wls_events.csv\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 4/4: ../datasets/netflow_day-02.csv ---\n",
            "  üìÑ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_netflow.csv\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "üéâüéâ Pipeline finished successfully! üéâüéâ\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import os\n",
        "\n",
        "def hex_to_int(val):\n",
        "    \"\"\"Safely convert hex or string numbers to integers.\"\"\"\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "def load_and_preprocess_unsupervised(csv_path, feature_cols, max_rows=10000):\n",
        "    \"\"\"Loads and preprocesses a single file for unsupervised learning.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  üìÑ Loaded {len(df)} rows from {csv_path}\")\n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  üìâ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "\n",
        "        print(f\"  ‚úÖ Final unsupervised dataset: {len(df)} rows\")\n",
        "        return df, df[feature_cols].values\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error in unsupervised loading for {csv_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def train_autoencoder(X_train_scaled):\n",
        "    \"\"\"Trains a standard autoencoder model.\"\"\"\n",
        "    print(\"  ü§ñ Training autoencoder...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(max(8, int(input_dim * 0.5)), activation=\"relu\")(encoder)\n",
        "    decoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(encoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_train_scaled, X_train_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"  ‚úÖ Autoencoder training complete.\")\n",
        "    return autoencoder\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "csv_list = [\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/cybersecurity_attacks.csv\",\n",
        "        \"features\": [\"Packet Length\", \"Source Port\", \"Destination Port\"],\n",
        "        \"out_csv\": \"classified_cybersecurity_attacks.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"supervised\", # This dataset uses the special supervised method\n",
        "        \"good_path\": \"../datasets/2good_reqff.csv\",\n",
        "        \"bad_path\": \"../datasets/2bad_reqff.csv\",\n",
        "        \"features\": [\"path_length\", \"body_length\", \"badwords_count\"],\n",
        "        \"out_csv\": \"classified_goodbad_requests.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/wls_day-02.csv\",\n",
        "        \"features\": [\"ProcessID\", \"ParentProcessID\", \"EventID\"],\n",
        "        \"out_csv\": \"classified_wls_events.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/netflow_day-02.csv\",\n",
        "        \"features\": [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"],\n",
        "        \"out_csv\": \"classified_netflow.csv\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# === EXECUTION PIPELINE ===\n",
        "print(\"üöÄ Starting Hybrid Anomaly Classification Pipeline\\n\")\n",
        "\n",
        "for i, config in enumerate(csv_list, 1):\n",
        "    print(f\"--- [‚öôÔ∏è] Processing Dataset {i}/{len(csv_list)}: {config.get('path') or config.get('good_path')} ---\")\n",
        "\n",
        "    df, final_score_col = None, None\n",
        "\n",
        "    if config[\"type\"] == \"supervised\":\n",
        "        try:\n",
        "            good_df = pd.read_csv(config['good_path'])\n",
        "            bad_df = pd.read_csv(config['bad_path'])\n",
        "            features = config['features']\n",
        "\n",
        "            X_train_good = good_df[features].values\n",
        "\n",
        "            df = pd.concat([good_df, bad_df], ignore_index=True)\n",
        "            X_all = df[features].values\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_good_scaled = scaler.fit_transform(X_train_good)\n",
        "            X_all_scaled = scaler.transform(X_all)\n",
        "\n",
        "            ae = train_autoencoder(X_train_good_scaled)\n",
        "            reconstructions = ae.predict(X_all_scaled)\n",
        "            df['Final_Score'] = np.mean(np.abs(X_all_scaled - reconstructions), axis=1)\n",
        "            print(\"  ‚úÖ Generated scores using supervised method.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error in supervised loading: {e}\")\n",
        "            continue\n",
        "\n",
        "    elif config[\"type\"] == \"unsupervised\":\n",
        "        df, X_features = load_and_preprocess_unsupervised(config['path'], config['features'])\n",
        "        if df is None:\n",
        "            print(f\"--- ‚è≠Ô∏è  Skipping {config['path']} ---\\n\")\n",
        "            continue\n",
        "\n",
        "        X_scaled = MinMaxScaler().fit_transform(X_features)\n",
        "        ae = train_autoencoder(X_scaled)\n",
        "        predictions = ae.predict(X_scaled)\n",
        "        df['Final_Score'] = np.mean(np.abs(X_scaled - predictions), axis=1)\n",
        "        print(\"  ‚úÖ Generated scores using unsupervised method.\")\n",
        "\n",
        "    # --- Classify into threat levels based on the Final_Score ---\n",
        "    if df is not None:\n",
        "        print(\"  üîé Classifying threats into levels...\")\n",
        "        level_3_threshold = df['Final_Score'].quantile(0.99)\n",
        "        level_2_threshold = df['Final_Score'].quantile(0.95)\n",
        "        level_1_threshold = df['Final_Score'].quantile(0.90)\n",
        "\n",
        "        conditions = [\n",
        "            df['Final_Score'] > level_3_threshold,\n",
        "            df['Final_Score'] > level_2_threshold,\n",
        "            df['Final_Score'] > level_1_threshold\n",
        "        ]\n",
        "        levels = ['Level 3 Threat (Critical)', 'Level 2 Threat (High)', 'Level 1 Threat (Medium)']\n",
        "        df['Threat_Level'] = np.select(conditions, levels, default='Not a Threat')\n",
        "\n",
        "        print(\"\\n  --- Threat Level Distribution ---\")\n",
        "        print(df['Threat_Level'].value_counts())\n",
        "        df.to_csv(config['out_csv'], index=False)\n",
        "        print(f\"\\n  üíæ Results saved to {config['out_csv']}\")\n",
        "        print(f\"\\n--- ‚úÖ Completed processing ---\\n\")\n",
        "\n",
        "print(\"üéâüéâ Pipeline finished successfully! üéâüéâ\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
