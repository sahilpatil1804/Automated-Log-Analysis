{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr5ew0DBbS4O",
        "outputId": "eb2d1fa9-9f7f-441f-8f3f-b7d9152c550d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to model.h5\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Data with Autoencoder Anomaly Scores:\n",
            "  method                    path body  single_q  double_q  dashes  braces  \\\n",
            "0    GET                       /  NaN         0         0       0       0   \n",
            "1    GET             /robots.txt  NaN         0         0       0       0   \n",
            "2    GET              /style.css  NaN         0         0       0       0   \n",
            "3    GET        /images/logo.gif  NaN         0         0       0       0   \n",
            "4    GET  /images/header_pic.jpg  NaN         0         0       0       0   \n",
            "\n",
            "   spaces  percentages  semicolons  angle_brackets  special_chars  \\\n",
            "0       0            0           0               0              0   \n",
            "1       0            0           0               0              0   \n",
            "2       0            0           0               0              0   \n",
            "3       0            0           0               0              0   \n",
            "4       0            0           0               0              0   \n",
            "\n",
            "   path_length  body_length  badwords_count class  anomaly_score_autoencoder  \n",
            "0            1            0               0  good                   1.200466  \n",
            "1           11            0               0  good                   0.966089  \n",
            "2           10            0               0  good                   0.989459  \n",
            "3           16            0               0  good                   0.849441  \n",
            "4           22            0               0  good                   0.709867  \n",
            "\n",
            "Sample of scores from the end of the file (bad requests):\n",
            "     method                                               path body  single_q  \\\n",
            "5792    GET  /my%20documents/JohnSmith/Bank%20Site%20Docume...  NaN         0   \n",
            "5793    GET  /my%20documents/JohnSmith/Bank%20Site%20Docume...  NaN         0   \n",
            "5794    GET  /my%20documents/JohnSmith/Bank%20Site%20Docume...  NaN         0   \n",
            "5795    GET                                        /sameDomain  NaN         0   \n",
            "5796    GET  /my%20documents/JohnSmith/Bank%20Site%20Docume...  NaN         0   \n",
            "\n",
            "      double_q  dashes  braces  spaces  percentages  semicolons  \\\n",
            "5792         0       0       0       3            0           0   \n",
            "5793         0       0       0       3            0           0   \n",
            "5794         0       0       0       3            0           0   \n",
            "5795         0       0       0       0            0           0   \n",
            "5796         0       0       0       3            0           0   \n",
            "\n",
            "      angle_brackets  special_chars  path_length  body_length  badwords_count  \\\n",
            "5792               0              0           57            0               0   \n",
            "5793               0              0           57            0               0   \n",
            "5794               0              0           57            0               0   \n",
            "5795               0              0           11            0               0   \n",
            "5796               0              0           57            0               0   \n",
            "\n",
            "     class  anomaly_score_autoencoder  \n",
            "5792   bad                   0.768330  \n",
            "5793   bad                   0.768330  \n",
            "5794   bad                   0.768330  \n",
            "5795   bad                   0.966089  \n",
            "5796   bad                   0.768330  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 1. Load and Prepare Data ---\n",
        "# Load both datasets\n",
        "good_df = pd.read_csv('/content/2good_reqff.csv')\n",
        "bad_df = pd.read_csv('/content/2bad_reqff.csv')\n",
        "\n",
        "# Define the features to be used\n",
        "feature_cols = ['path_length', 'body_length', 'badwords_count']\n",
        "\n",
        "# Create the training data using ONLY good requests\n",
        "X_train_good = good_df[feature_cols].values\n",
        "\n",
        "# Combine all data for testing later\n",
        "combined_df = pd.concat([good_df, bad_df], ignore_index=True)\n",
        "X_all = combined_df[feature_cols].values\n",
        "\n",
        "# Scale the features. It's crucial to fit the scaler ONLY on the good data.\n",
        "scaler = StandardScaler()\n",
        "X_train_good_scaled = scaler.fit_transform(X_train_good)\n",
        "X_all_scaled = scaler.transform(X_all) # Apply the same scaling to all data\n",
        "\n",
        "# --- 2. Build and Train the Autoencoder ---\n",
        "autoencoder = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(units=2, activation='relu', input_shape=(X_all_scaled.shape[1],)),\n",
        "  tf.keras.layers.Dense(units=X_all_scaled.shape[1], activation='sigmoid')\n",
        "])\n",
        "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError())\n",
        "\n",
        "\n",
        "# Train the model on the scaled GOOD data\n",
        "autoencoder.fit(X_train_good_scaled, X_train_good_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "\n",
        "autoencoder.save(\"model.h5\")\n",
        "print(\"Model saved to model.h5\")\n",
        "\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# --- 3. Generate Anomaly Scores ---\n",
        "# Get the model's reconstructions of ALL data\n",
        "reconstructions = autoencoder.predict(X_all_scaled)\n",
        "\n",
        "# Calculate the Mean Absolute Error between the original and reconstructed data\n",
        "# This error is our anomaly score\n",
        "anomaly_scores_ae = np.mean(np.abs(X_all_scaled - reconstructions), axis=1)\n",
        "\n",
        "# Add the scores to our combined dataframe\n",
        "combined_df['anomaly_score_autoencoder'] = anomaly_scores_ae\n",
        "\n",
        "print(\"Data with Autoencoder Anomaly Scores:\")\n",
        "print(combined_df.head())\n",
        "print(\"\\nSample of scores from the end of the file (bad requests):\")\n",
        "print(combined_df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC45SbZPwCdE",
        "outputId": "93192790-7373-4de1-ab45-46f87f7dae74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold set to: 1.2\n",
            "True Negatives (Good logs identified as Good): 278\n",
            "False Positives (Good logs flagged as Bad): 9\n",
            "False Negatives (Bad logs missed): 1419\n",
            "True Positives (Bad logs caught): 4091\n",
            "\n",
            "False Positive Rate: 3.14%\n",
            "Accuracy: 75.37%\n",
            "Detection Rate (Recall): 74.25%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# --- 1. Choose your threshold based on the plot ---\n",
        "threshold = 1.2\n",
        "\n",
        "# --- 2. Apply the threshold to get final predictions ---\n",
        "# If score > threshold, it's an anomaly (1), otherwise it's normal (0)\n",
        "combined_df['prediction'] = (combined_df['anomaly_score_autoencoder'] > threshold).astype(int)\n",
        "\n",
        "# Create the true labels for comparison (bad=1, good=0)\n",
        "combined_df['true_label'] = (combined_df['class'] == 'bad').astype(int)\n",
        "\n",
        "# --- 3. Calculate Final Metrics ---\n",
        "# Generate the confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(combined_df['true_label'], combined_df['prediction']).ravel()\n",
        "\n",
        "print(f\"Threshold set to: {threshold}\")\n",
        "print(f\"True Negatives (Good logs identified as Good): {tn}\")\n",
        "print(f\"False Positives (Good logs flagged as Bad): {fp}\")\n",
        "print(f\"False Negatives (Bad logs missed): {fn}\")\n",
        "print(f\"True Positives (Bad logs caught): {tp}\")\n",
        "\n",
        "# False Positive Rate (FPR) -> Your project goal is <5%\n",
        "fpr = fp / (fp + tn)\n",
        "print(f\"\\nFalse Positive Rate: {fpr:.2%}\")\n",
        "\n",
        "# Accuracy -> Overall correctness\n",
        "accuracy = accuracy_score(combined_df['true_label'], combined_df['prediction'])\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Detection Rate (Recall) -> Percentage of bad logs you successfully caught\n",
        "detection_rate = tp / (tp + fn)\n",
        "print(f\"Detection Rate (Recall): {detection_rate:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc1sC4djHOJZ",
        "outputId": "23e2503a-c84b-40ac-9323-815409d01fac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading autoencoder...\n",
            "[INFO] Computing reconstruction errors...\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "[INFO] Training surrogate model (RandomForest)...\n",
            "[✅] Saved model as surrogate_rf_model.pkl\n",
            "[INFO] Running fast SHAP explainability...\n"
          ]
        }
      ],
      "source": [
        "AUTOENCODER_PATH = \"model.h5\"\n",
        "GOOD_CSV = \"2good_reqff.csv\"\n",
        "BAD_CSV = \"2bad_reqff.csv\"\n",
        "SCALER_PATH = \"scaler.pkl\"\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "\n",
        "# Load data\n",
        "\n",
        "df_good = pd.read_csv(GOOD_CSV)\n",
        "df_bad = pd.read_csv(BAD_CSV)\n",
        "\n",
        "df_combined = pd.concat([df_good, df_bad], axis=0).reset_index(drop=True)\n",
        "feature_cols = ['path_length', 'body_length', 'badwords_count']\n",
        "df_features = df_combined[feature_cols].copy()\n",
        "df_features = df_features.dropna()\n",
        "data = df_features.astype(np.float32).values\n",
        "\n",
        "\n",
        "# Load trained autoencoder\n",
        "\n",
        "print(\"[INFO] Loading autoencoder...\")\n",
        "autoencoder = tf.keras.models.load_model(\"model.h5\")\n",
        "\n",
        "# Compute reconstruction error per feature\n",
        "\n",
        "print(\"[INFO] Computing reconstruction errors...\")\n",
        "reconstructed = autoencoder.predict(data)\n",
        "reconstruction_errors = np.square(data - reconstructed)  # shape = (n_samples, n_features)\n",
        "total_error = reconstruction_errors.mean(axis=1)\n",
        "\n",
        "# === 1. Train surrogate model ===\n",
        "print(\"[INFO] Training surrogate model (RandomForest)...\")\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(data, total_error)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(rf, \"surrogate_rf_model.pkl\")\n",
        "print(\"[✅] Saved model as surrogate_rf_model.pkl\")\n",
        "\n",
        "# === 2. Use only top 200 anomalies ===\n",
        "top_n = 200\n",
        "top_indices = np.argsort(total_error)[-top_n:]\n",
        "X_top = data[top_indices]\n",
        "errors_top = total_error[top_indices]\n",
        "\n",
        "# === 3. Run SHAP with TreeExplainer using approximate method ===\n",
        "print(\"[INFO] Running fast SHAP explainability...\")\n",
        "explainer = shap.TreeExplainer(rf, data=X_top, feature_perturbation=\"interventional\")\n",
        "shap_values = explainer.shap_values(X_top, approximate=True)\n",
        "\n",
        "# === 4. SHAP dataframe ===\n",
        "shap_df = pd.DataFrame(shap_values, columns=feature_cols)\n",
        "shap_df[\"anomaly_score\"] = errors_top\n",
        "\n",
        "# Optional: original data slice for joining\n",
        "original_top = df_combined.iloc[top_indices].reset_index(drop=True)\n",
        "final_df = pd.concat([original_top.reset_index(drop=True), shap_df.add_prefix(\"shap_\")], axis=1)\n",
        "\n",
        "# === 5. Save to CSV ===\n",
        "final_df.to_csv(\"shap_explanations_goodbad.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elI0F18OOBxJ",
        "outputId": "fc261ade-5518-4eca-8464-748be0a1b3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[✅] Cleaned + Scaled Shape: (2604, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "✅ Saved shap_explanations_top200.csv ✅\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import shap\n",
        "\n",
        "\n",
        "# Load your CSV (replace path as needed)\n",
        "df = pd.read_csv(\"/content/wls_day-02.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Select numeric + useful columns\n",
        "cols = [\"EventID\", \"LogonID\", \"ParentProcessID\", \"ProcessID\"]\n",
        "df_selected = df[cols].copy()\n",
        "\n",
        "# Convert hex-like strings to integers\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "for col in [\"LogonID\", \"ParentProcessID\", \"ProcessID\"]:\n",
        "    df_selected[col] = df_selected[col].apply(hex_to_int)\n",
        "\n",
        "# Drop missing values\n",
        "df_selected.dropna(inplace=True)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_selected)\n",
        "\n",
        "print(\"[✅] Cleaned + Scaled Shape:\", X_scaled.shape)\n",
        "\n",
        "\n",
        "# Build autoencoder model\n",
        "autoencoder = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(X_scaled.shape[1], activation='linear')\n",
        "])\n",
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "\n",
        "# Get reconstructions\n",
        "reconstructions = autoencoder.predict(X_scaled)\n",
        "anomaly_scores = np.mean(np.abs(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_scaled, anomaly_scores)\n",
        "\n",
        "\n",
        "\n",
        "# === 1. TreeExplainer with fast approximation ===\n",
        "explainer = shap.TreeExplainer(\n",
        "    rf_model,\n",
        "    data=X_scaled,\n",
        "    feature_perturbation=\"interventional\",\n",
        "    model_output=\"raw\"\n",
        ")\n",
        "\n",
        "# === 2. Focus on top 200 anomalies to save time ===\n",
        "top_anomalies = np.argsort(anomaly_scores)[-200:]  # take worst 200 logs\n",
        "X_top = X_scaled[top_anomalies]\n",
        "\n",
        "# Approximate SHAP values (faster)\n",
        "shap_values = explainer.shap_values(X_top, approximate=True)\n",
        "\n",
        "# === 3. Prepare DataFrames ===\n",
        "# SHAP DataFrame\n",
        "shap_df = pd.DataFrame(shap_values, columns=cols)\n",
        "shap_df[\"anomaly_score\"] = anomaly_scores[top_anomalies]\n",
        "\n",
        "# Original data subset\n",
        "df_top = df_selected.iloc[top_anomalies].reset_index(drop=True)\n",
        "df_top[\"anomaly_score\"] = anomaly_scores[top_anomalies]\n",
        "\n",
        "# Final output with original + SHAP\n",
        "final_df = pd.concat([df_top, shap_df.add_prefix(\"shap_\")], axis=1)\n",
        "\n",
        "# === 4. Save CSV ===\n",
        "final_df.to_csv(\"shap_explanations_top200.csv\", index=False)\n",
        "print(\"✅ Saved shap_explanations_top200.csv ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYBmkVQ3KuZP",
        "outputId": "fe1f42b7-c447-4af5-de18-393f60185fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[✅] Inserted 200 records from /content/shap_explanations_top200.csv\n",
            "[✅] Inserted 200 records from /content/shap_explanations_goodbad.csv\n",
            "\n",
            "[🏁 DONE] Inserted total 400 SHAP logs into MongoDB Atlas!\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# List of CSV files to insert\n",
        "csv_files = [\n",
        "    \"/content/shap_explanations_top200.csv\",\n",
        "    \"/content/shap_explanations_goodbad.csv\"\n",
        "]\n",
        "\n",
        "# MongoDB URI\n",
        "uri = \"ENTER YOUR KEY",
        "client = MongoClient(uri)\n",
        "\n",
        "# Target DB and collection\n",
        "db = client[\"log_analysis\"]\n",
        "collection = db[\"shap_explanations\"]\n",
        "\n",
        "# Insert all files\n",
        "total_inserted = 0\n",
        "for csv in csv_files:\n",
        "    df = pd.read_csv(csv)\n",
        "    records = df.to_dict(orient=\"records\")\n",
        "    if records:  # only insert if not empty\n",
        "        collection.insert_many(records)\n",
        "        total_inserted += len(records)\n",
        "        print(f\"[✅] Inserted {len(records)} records from {csv}\")\n",
        "    else:\n",
        "        print(f\"[⚠️] Skipped {csv} — empty or invalid\")\n",
        "\n",
        "print(f\"\\n[🏁 DONE] Inserted total {total_inserted} SHAP logs into MongoDB Atlas!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
