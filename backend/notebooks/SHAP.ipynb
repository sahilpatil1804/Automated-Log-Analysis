{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbY-eTHAIvv",
        "outputId": "d765f8d4-82c7-47fe-8414-27293e405fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Anomaly Detection Pipeline\n",
            "\n",
            "[‚öôÔ∏è] Processing Dataset 1/3: ../datasets/2good_reqff.csv\n",
            "     Features: ['path_length', 'body_length', 'badwords_count']\n",
            "  üìÑ Loaded 287 rows from ../datasets/2good_reqff.csv\n",
            "  üßπ Removed 0 rows with missing values\n",
            "  ‚úÖ Final dataset: 287 rows, 3 features\n",
            "  üìè Scaling features...\n",
            "  ü§ñ Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Autoencoder training complete\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.3580, Max: 7.0644, Mean: 0.6181\n",
            "  üå≤ Training Random Forest...\n",
            "  üîç Generating SHAP explanations for top 200 anomalies...\n",
            "  ‚úÖ SHAP explanations generated\n",
            "  üíæ All models saved to saved_models/goodbad\n",
            "  üíæ Saved results to shap_explanations_goodbad.csv\n",
            "  üìã Sample Results (Top 5 Anomalies):\n",
            "method                                path body  single_q  double_q  dashes  braces  spaces  percentages  semicolons  angle_brackets  special_chars  path_length  body_length  badwords_count class  shap_path_length  shap_body_length  shap_badwords_count  shap_anomaly_score\n",
            "   GET /index.jsp?content=inside_press.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_press.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "\n",
            "  üìä Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 7.0644\n",
            "     - Average anomaly score: 0.7195\n",
            "\n",
            "‚úÖ Completed processing ../datasets/2good_reqff.csv\n",
            "==================================================\n",
            "\n",
            "[‚öôÔ∏è] Processing Dataset 2/3: ../datasets/wls_day-02.csv\n",
            "     Features: ['ProcessID', 'ParentProcessID', 'EventID']\n",
            "  üìÑ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  üßπ Removed 2396 rows with missing values\n",
            "  ‚úÖ Final dataset: 2604 rows, 3 features\n",
            "  üìè Scaling features...\n",
            "  ü§ñ Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Autoencoder training complete\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.0539, Max: 6.6316, Mean: 0.3011\n",
            "  üå≤ Training Random Forest...\n",
            "  üîç Generating SHAP explanations for top 200 anomalies...\n",
            "  ‚úÖ SHAP explanations generated\n",
            "  üíæ All models saved to saved_models/network\n",
            "  üíæ Saved results to shap_explanations_network.csv\n",
            "  üìã Sample Results (Top 5 Anomalies):\n",
            "   UserName  EventID    LogHost   LogonID DomainName ParentProcessName  ParentProcessID    ProcessName  Time  ProcessID LogonTypeDescription Source AuthenticationPackage  LogonType Destination SubjectUserName SubjectLogonID SubjectDomainName Status ServiceName FailureReason  shap_ProcessID  shap_ParentProcessID  shap_EventID  shap_anomaly_score\n",
            "Comp655648$   4688.0 Comp655648     0x3e7  Domain001          services            704.0   rundll32.exe 86400    21324.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.608672             -0.196779           0.0            0.576615\n",
            "Comp916004$   4688.0 Comp916004     0x3e7  Domain001        Proc950869           5804.0    cscript.exe 86400    19568.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.443401             -0.358333           0.0            0.577221\n",
            " User886483   4688.0 Comp350505 0x34d282e  Domain001        Proc412499           5196.0 Proc407594.exe 86400     1424.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.384934             -0.414691           0.0            0.578973\n",
            "Comp916004$   4688.0 Comp916004     0x3e7  Domain001        Proc950869           5804.0    cscript.exe 86400    19648.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.435583             -0.363031           0.0            0.579735\n",
            "Comp028683$   4688.0 Comp028683     0x3e7  Domain001          services            844.0    svchost.exe 86400    21864.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.489617             -0.300552           0.0            0.581163\n",
            "\n",
            "  üìä Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 6.6316\n",
            "     - Average anomaly score: 1.3544\n",
            "\n",
            "‚úÖ Completed processing ../datasets/wls_day-02.csv\n",
            "==================================================\n",
            "\n",
            "[‚öôÔ∏è] Processing Dataset 3/3: ../datasets/netflow_day-02.csv\n",
            "     Features: ['Duration', 'SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes']\n",
            "  üìÑ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  üìâ Sampled down to 5000 rows\n",
            "  üßπ Removed 0 rows with missing values\n",
            "  ‚úÖ Final dataset: 5000 rows, 5 features\n",
            "  üìè Scaling features...\n",
            "  ü§ñ Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Autoencoder training complete\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.0198, Max: 31.6918, Mean: 0.1710\n",
            "  üå≤ Training Random Forest...\n",
            "  üîç Generating SHAP explanations for top 200 anomalies...\n",
            "  ‚úÖ SHAP explanations generated\n",
            "  üíæ All models saved to saved_models/host\n",
            "  üíæ Saved results to shap_explanations_host.csv\n",
            "  üìã Sample Results (Top 5 Anomalies):\n",
            "  Time  Duration  SrcDevice  DstDevice  Protocol   SrcPort   DstPort  SrcPackets  DstPackets  SrcBytes  DstBytes  shap_Duration  shap_SrcPackets  shap_DstPackets  shap_SrcBytes  shap_DstBytes  shap_anomaly_score\n",
            "121173  689622.0 Comp989948 Comp730289        17       123       123      5224.0         0.0  397024.0       0.0      -0.368661        -0.918774        -0.072914      -0.077712      -0.061812            0.363811\n",
            "121053  752556.0 Comp124494 Comp253429         6 Port94511       445         0.0      6616.0       0.0  595575.0      -0.337807        -0.868480        -0.179600      -0.048423      -0.063130            0.363950\n",
            "122404  678738.0 Comp265246 Comp186884         1 Port15379 Port15379      1129.0         0.0   51934.0       0.0      -0.377774        -0.890739        -0.072914      -0.088125      -0.064897            0.364052\n",
            "121112  681280.0 Comp279702 Comp738970        17       137       137      1980.0         0.0  169776.0       0.0      -0.376939        -0.891065        -0.072903      -0.086837      -0.064872            0.367635\n",
            "121324  700319.0 Comp171178 Comp469322        17 Port73388       162      4235.0         0.0 1079359.0       0.0      -0.374496        -0.893781        -0.072914      -0.089426      -0.064872            0.368550\n",
            "\n",
            "  üìä Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 31.6918\n",
            "     - Average anomaly score: 1.9228\n",
            "\n",
            "‚úÖ Completed processing ../datasets/netflow_day-02.csv\n",
            "==================================================\n",
            "\n",
            "üéâ Pipeline completed successfully!\n",
            "üìÅ All models saved in 'saved_models' directory\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import shap\n",
        "import warnings\n",
        "import joblib\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')\n",
        "\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def load_and_preprocess(csv_path, feature_cols, max_rows=5000):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  üìÑ Loaded {len(df)} rows from {csv_path}\")\n",
        "        \n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  üìâ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        initial_rows = len(df)\n",
        "        df = df.dropna(subset=feature_cols)\n",
        "        print(f\"  üßπ Removed {initial_rows - len(df)} rows with missing values\")\n",
        "\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "\n",
        "        df = df.dropna(subset=feature_cols)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "        \n",
        "        print(f\"  ‚úÖ Final dataset: {len(df)} rows, {len(feature_cols)} features\")\n",
        "        return df, df[feature_cols].values\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ‚ùå File not found: {csv_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error processing {csv_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def train_autoencoder(X_train):\n",
        "    print(\"  ü§ñ Training autoencoder...\")\n",
        "    model = Sequential([\n",
        "        Dense(2, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(X_train.shape[1], activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    model.fit(X_train, X_train, epochs=20, batch_size=32, verbose=0)\n",
        "    print(\"  ‚úÖ Autoencoder training complete\")\n",
        "    return model\n",
        "\n",
        "def compute_anomaly_scores(autoencoder, X):\n",
        "    print(\"  üìä Computing anomaly scores...\")\n",
        "    recon = autoencoder.predict(X, verbose=0)\n",
        "    scores = np.mean(np.abs(X - recon), axis=1)\n",
        "    print(f\"  üìà Anomaly scores - Min: {scores.min():.4f}, Max: {scores.max():.4f}, Mean: {scores.mean():.4f}\")\n",
        "    return scores\n",
        "\n",
        "def explain_with_shap(model, X, feature_names, scores, top_n=200):\n",
        "    print(f\"  üîç Generating SHAP explanations for top {top_n} anomalies...\")\n",
        "    top_idx = np.argsort(scores)[-top_n:]\n",
        "    X_top = X[top_idx]\n",
        "    scores_top = scores[top_idx]\n",
        "    \n",
        "    explainer = shap.TreeExplainer(model, data=X_top, feature_perturbation=\"interventional\")\n",
        "    shap_values = explainer.shap_values(X_top, approximate=True)\n",
        "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
        "    shap_df[\"anomaly_score\"] = scores_top\n",
        "    \n",
        "    print(\"  ‚úÖ SHAP explanations generated\")\n",
        "    return shap_df, top_idx, explainer\n",
        "\n",
        "def save_models(dataset_name, autoencoder, scaler, rf_model, explainer, feature_names):\n",
        "    \"\"\"Save all models for a specific dataset\"\"\"\n",
        "    model_dir = f\"saved_models/{dataset_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save autoencoder (Keras model)\n",
        "    autoencoder.save(f\"{model_dir}/autoencoder.keras\")\n",
        "    \n",
        "    # Save scaler and random forest (sklearn models)\n",
        "    joblib.dump(scaler, f\"{model_dir}/scaler.pkl\")\n",
        "    joblib.dump(rf_model, f\"{model_dir}/random_forest.pkl\")\n",
        "    \n",
        "    # Save SHAP explainer and feature names\n",
        "    with open(f\"{model_dir}/shap_explainer.pkl\", 'wb') as f:\n",
        "        pickle.dump(explainer, f)\n",
        "    \n",
        "    with open(f\"{model_dir}/feature_names.pkl\", 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "    \n",
        "    print(f\"  üíæ All models saved to {model_dir}\")\n",
        "\n",
        "# === CONFIG ===\n",
        "csv_list = [\n",
        "    (\"../datasets/2good_reqff.csv\", [\"path_length\", \"body_length\", \"badwords_count\"], \"shap_explanations_goodbad.csv\", \"goodbad\"),\n",
        "    (\"../datasets/wls_day-02.csv\", [\"ProcessID\", \"ParentProcessID\", \"EventID\"], \"shap_explanations_network.csv\", \"network\"),\n",
        "    (\"../datasets/netflow_day-02.csv\", [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"], \"shap_explanations_host.csv\", \"host\")\n",
        "]\n",
        "\n",
        "# Create main models directory\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# === PIPELINE ===\n",
        "print(\"üöÄ Starting Anomaly Detection Pipeline\\n\")\n",
        "\n",
        "for i, (path, features, out_csv, model_name) in enumerate(csv_list, 1):\n",
        "    print(f\"[‚öôÔ∏è] Processing Dataset {i}/3: {path}\")\n",
        "    print(f\"     Features: {features}\")\n",
        "    \n",
        "    df, X = load_and_preprocess(path, features)\n",
        "    if df is None:\n",
        "        print(\"     ‚è≠Ô∏è  Skipping to next dataset\\n\")\n",
        "        continue\n",
        "    \n",
        "    print(\"  üìè Scaling features...\")\n",
        "    scaler = StandardScaler().fit(X)\n",
        "    X_scaled = scaler.transform(X)\n",
        "    \n",
        "    ae = train_autoencoder(X_scaled)\n",
        "    scores = compute_anomaly_scores(ae, X_scaled)\n",
        "    \n",
        "    print(\"  üå≤ Training Random Forest...\")\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_scaled, scores)\n",
        "    \n",
        "    shap_df, top_idx, explainer = explain_with_shap(rf, X_scaled, features, scores)\n",
        "    \n",
        "    # Save all models\n",
        "    save_models(model_name, ae, scaler, rf, explainer, features)\n",
        "    \n",
        "    final_df = pd.concat([\n",
        "        df.iloc[top_idx].reset_index(drop=True), \n",
        "        shap_df.add_prefix(\"shap_\")\n",
        "    ], axis=1)\n",
        "    \n",
        "    final_df.to_csv(out_csv, index=False)\n",
        "    print(f\"  üíæ Saved results to {out_csv}\")\n",
        "    \n",
        "    print(f\"  üìã Sample Results (Top 5 Anomalies):\")\n",
        "    print(final_df.head().to_string(index=False))\n",
        "    print(f\"\\n  üìä Summary Statistics:\")\n",
        "    print(f\"     - Total anomalies analyzed: {len(final_df)}\")\n",
        "    print(f\"     - Highest anomaly score: {final_df['shap_anomaly_score'].max():.4f}\")\n",
        "    print(f\"     - Average anomaly score: {final_df['shap_anomaly_score'].mean():.4f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Completed processing {path}\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"üéâ Pipeline completed successfully!\")\n",
        "print(\"üìÅ All models saved in 'saved_models' directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "25sSXNVxS1q2",
        "outputId": "dc303d30-008b-41a6-f049-baf0f445f12a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'wls_day-02.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwls_day-02.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update with your path if needed\u001b[39;00m\n\u001b[32m     11\u001b[39m feature_cols = [\u001b[33m\"\u001b[39m\u001b[33mProcessID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mParentProcessID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLogonID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEventID\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Convert hex to int\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'wls_day-02.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"wls_day-02.csv\")  # update with your path if needed\n",
        "feature_cols = [\"ProcessID\", \"ParentProcessID\", \"LogonID\", \"EventID\"]\n",
        "\n",
        "# Convert hex to int\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "for col in feature_cols:\n",
        "    df[col] = df[col].apply(hex_to_int)\n",
        "\n",
        "df.dropna(subset=feature_cols, inplace=True)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X = df[feature_cols].astype(np.float32)\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Autoencoder\n",
        "model = Sequential([\n",
        "    Dense(2, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(X_scaled.shape[1], activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "history = model.fit(X_scaled, X_scaled, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Reconstruction error\n",
        "recon = model.predict(X_scaled, verbose=0)\n",
        "errors = np.mean(np.abs(X_scaled - recon), axis=1)\n",
        "\n",
        "# Assume top 5% anomalies\n",
        "threshold = np.percentile(errors, 45)\n",
        "preds = (errors > threshold).astype(int)\n",
        "labels = np.zeros(len(errors))\n",
        "labels[errors > threshold] = 1\n",
        "\n",
        "# Metrics\n",
        "print(\"Precision:\", precision_score(labels, preds))\n",
        "print(\"Recall:\", recall_score(labels, preds))\n",
        "print(\"F1:\", f1_score(labels, preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(labels, errors))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(labels, preds))\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Autoencoder Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MAE Loss\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww6Md0lUD7-H",
        "outputId": "aebb9475-c0a5-4453-ef91-c35d689db002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': ObjectId('6894ffdd874daee6c347819d'), 'path_length': -4829.610679602709, 'body_length': -5071.62602397515, 'badwords_count': -746.8810707360627, 'anomaly_score': 0.07648983, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c347819e'), 'path_length': -4799.939513546723, 'body_length': -5069.499950897789, 'badwords_count': -745.4170540314017, 'anomaly_score': 33.337746, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c347819f'), 'path_length': -4810.467175360215, 'body_length': -5064.038138792648, 'badwords_count': -746.6811775439102, 'anomaly_score': 27.007772, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c34781a0'), 'path_length': -4718.863101373702, 'body_length': -5109.013793914081, 'badwords_count': -745.3171399727747, 'anomaly_score': 75.00023, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c34781a1'), 'path_length': -4669.151650568704, 'body_length': -5087.3096556579385, 'badwords_count': -744.7329579157633, 'anomaly_score': 147.0, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "uri = \"ENTER YOUR KEY",
        "client = MongoClient(uri)\n",
        "\n",
        "db = client[\"log_analysis\"]\n",
        "collection = db[\"shap_explanations\"]\n",
        "\n",
        "# Print 5 documents\n",
        "for doc in collection.find().limit(5):\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-09 19:47:28.198921: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-08-09 19:47:28.301752: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-08-09 19:47:28.334820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754749048.408382  115493 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754749048.425547  115493 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754749048.563370  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563399  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563402  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563405  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-09 19:47:28.573011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Hybrid Anomaly Classification Pipeline\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 1/4: ../datasets/cybersecurity_attacks.csv ---\n",
            "  üìÑ Loaded 40000 rows from ../datasets/cybersecurity_attacks.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-09 19:47:31.716218: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
            "2025-08-09 19:47:31.716261: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
            "2025-08-09 19:47:31.716268: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: fedora\n",
            "2025-08-09 19:47:31.716274: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: fedora\n",
            "2025-08-09 19:47:31.716410: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 575.64.5\n",
            "2025-08-09 19:47:31.716452: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 575.64.5\n",
            "2025-08-09 19:47:31.716460: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 575.64.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_cybersecurity_attacks.csv\n",
            "  üíæ All models saved to saved_models/cybersecurity_attacks\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 2/4: ../datasets/2good_reqff.csv ---\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step\n",
            "  ‚úÖ Generated scores using supervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 5217\n",
            "Level 1 Threat (Medium)       291\n",
            "Level 2 Threat (High)         231\n",
            "Level 3 Threat (Critical)      58\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_goodbad_requests.csv\n",
            "  üíæ All models saved to saved_models/goodbad_requests\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 3/4: ../datasets/wls_day-02.csv ---\n",
            "  üìÑ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  ‚úÖ Final unsupervised dataset: 2604 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 2343\n",
            "Level 1 Threat (Medium)       130\n",
            "Level 2 Threat (High)         104\n",
            "Level 3 Threat (Critical)      27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_wls_events.csv\n",
            "  üíæ All models saved to saved_models/wls_events\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 4/4: ../datasets/netflow_day-02.csv ---\n",
            "  üìÑ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_netflow.csv\n",
            "  üíæ All models saved to saved_models/netflow_analysis\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "üéâüéâ Pipeline finished successfully! üéâüéâ\n",
            "üìÅ All models saved in 'saved_models' directory\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import os\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "\n",
        "def hex_to_int(val):\n",
        "    \"\"\"Safely convert hex or string numbers to integers.\"\"\"\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def load_and_preprocess_unsupervised(csv_path, feature_cols, max_rows=10000):\n",
        "    \"\"\"Loads and preprocesses a single file for unsupervised learning.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  üìÑ Loaded {len(df)} rows from {csv_path}\")\n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  üìâ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "\n",
        "        print(f\"  ‚úÖ Final unsupervised dataset: {len(df)} rows\")\n",
        "        return df, df[feature_cols].values\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error in unsupervised loading for {csv_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def train_autoencoder(X_train_scaled):\n",
        "    \"\"\"Trains a standard autoencoder model.\"\"\"\n",
        "    print(\"  ü§ñ Training autoencoder...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(max(8, int(input_dim * 0.5)), activation=\"relu\")(encoder)\n",
        "    decoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(encoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_train_scaled, X_train_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"  ‚úÖ Autoencoder training complete.\")\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def save_models(dataset_name, autoencoder, scaler, feature_names):\n",
        "    \"\"\"Save autoencoder and scaler for a specific dataset\"\"\"\n",
        "    model_dir = f\"saved_models/{dataset_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save autoencoder (Keras model)\n",
        "    autoencoder.save(f\"{model_dir}/autoencoder.keras\")\n",
        "    \n",
        "    # Save scaler\n",
        "    joblib.dump(scaler, f\"{model_dir}/scaler.pkl\")\n",
        "    \n",
        "    # Save feature names\n",
        "    with open(f\"{model_dir}/feature_names.pkl\", 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "    \n",
        "    print(f\"  üíæ All models saved to {model_dir}\")\n",
        "\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "csv_list = [\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/cybersecurity_attacks.csv\",\n",
        "        \"features\": [\"Packet Length\", \"Source Port\", \"Destination Port\"],\n",
        "        \"out_csv\": \"classified_cybersecurity_attacks.csv\",\n",
        "        \"model_name\": \"cybersecurity_attacks\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"supervised\", # This dataset uses the special supervised method\n",
        "        \"good_path\": \"../datasets/2good_reqff.csv\",\n",
        "        \"bad_path\": \"../datasets/2bad_reqff.csv\",\n",
        "        \"features\": [\"path_length\", \"body_length\", \"badwords_count\"],\n",
        "        \"out_csv\": \"classified_goodbad_requests.csv\",\n",
        "        \"model_name\": \"goodbad_requests\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/wls_day-02.csv\",\n",
        "        \"features\": [\"ProcessID\", \"ParentProcessID\", \"EventID\"],\n",
        "        \"out_csv\": \"classified_wls_events.csv\",\n",
        "        \"model_name\": \"wls_events\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/netflow_day-02.csv\",\n",
        "        \"features\": [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"],\n",
        "        \"out_csv\": \"classified_netflow.csv\",\n",
        "        \"model_name\": \"netflow_analysis\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create main models directory\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# === EXECUTION PIPELINE ===\n",
        "print(\"üöÄ Starting Hybrid Anomaly Classification Pipeline\\n\")\n",
        "\n",
        "for i, config in enumerate(csv_list, 1):\n",
        "    print(f\"--- [‚öôÔ∏è] Processing Dataset {i}/{len(csv_list)}: {config.get('path') or config.get('good_path')} ---\")\n",
        "\n",
        "    df, final_score_col = None, None\n",
        "    autoencoder, scaler = None, None\n",
        "\n",
        "    if config[\"type\"] == \"supervised\":\n",
        "        try:\n",
        "            good_df = pd.read_csv(config['good_path'])\n",
        "            bad_df = pd.read_csv(config['bad_path'])\n",
        "            features = config['features']\n",
        "\n",
        "            X_train_good = good_df[features].values\n",
        "\n",
        "            df = pd.concat([good_df, bad_df], ignore_index=True)\n",
        "            X_all = df[features].values\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_good_scaled = scaler.fit_transform(X_train_good)\n",
        "            X_all_scaled = scaler.transform(X_all)\n",
        "\n",
        "            autoencoder = train_autoencoder(X_train_good_scaled)\n",
        "            reconstructions = autoencoder.predict(X_all_scaled)\n",
        "            df['Final_Score'] = np.mean(np.abs(X_all_scaled - reconstructions), axis=1)\n",
        "            print(\"  ‚úÖ Generated scores using supervised method.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error in supervised loading: {e}\")\n",
        "            continue\n",
        "\n",
        "    elif config[\"type\"] == \"unsupervised\":\n",
        "        df, X_features = load_and_preprocess_unsupervised(config['path'], config['features'])\n",
        "        if df is None:\n",
        "            print(f\"--- ‚è≠Ô∏è  Skipping {config['path']} ---\\n\")\n",
        "            continue\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        X_scaled = scaler.fit_transform(X_features)\n",
        "        autoencoder = train_autoencoder(X_scaled)\n",
        "        predictions = autoencoder.predict(X_scaled)\n",
        "        df['Final_Score'] = np.mean(np.abs(X_scaled - predictions), axis=1)\n",
        "        print(\"  ‚úÖ Generated scores using unsupervised method.\")\n",
        "\n",
        "    # --- Classify into threat levels based on the Final_Score ---\n",
        "    if df is not None:\n",
        "        print(\"  üîé Classifying threats into levels...\")\n",
        "        level_3_threshold = df['Final_Score'].quantile(0.99)\n",
        "        level_2_threshold = df['Final_Score'].quantile(0.95)\n",
        "        level_1_threshold = df['Final_Score'].quantile(0.90)\n",
        "\n",
        "        conditions = [\n",
        "            df['Final_Score'] > level_3_threshold,\n",
        "            df['Final_Score'] > level_2_threshold,\n",
        "            df['Final_Score'] > level_1_threshold\n",
        "        ]\n",
        "        levels = ['Level 3 Threat (Critical)', 'Level 2 Threat (High)', 'Level 1 Threat (Medium)']\n",
        "        df['Threat_Level'] = np.select(conditions, levels, default='Not a Threat')\n",
        "\n",
        "        print(\"\\n  --- Threat Level Distribution ---\")\n",
        "        print(df['Threat_Level'].value_counts())\n",
        "        df.to_csv(config['out_csv'], index=False)\n",
        "        print(f\"\\n  üíæ Results saved to {config['out_csv']}\")\n",
        "        \n",
        "        # Save the trained models\n",
        "        if autoencoder is not None and scaler is not None:\n",
        "            save_models(config['model_name'], autoencoder, scaler, config['features'])\n",
        "        \n",
        "        print(f\"\\n--- ‚úÖ Completed processing ---\\n\")\n",
        "\n",
        "print(\"üéâüéâ Pipeline finished successfully! üéâüéâ\")\n",
        "print(\"üìÅ All models saved in 'saved_models' directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-09 19:47:28.198921: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-08-09 19:47:28.301752: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-08-09 19:47:28.334820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754749048.408382  115493 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754749048.425547  115493 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754749048.563370  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563399  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563402  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754749048.563405  115493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-09 19:47:28.573011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Hybrid Anomaly Classification Pipeline\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 1/4: ../datasets/cybersecurity_attacks.csv ---\n",
            "  üìÑ Loaded 40000 rows from ../datasets/cybersecurity_attacks.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-09 19:47:31.716218: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
            "2025-08-09 19:47:31.716261: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
            "2025-08-09 19:47:31.716268: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: fedora\n",
            "2025-08-09 19:47:31.716274: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: fedora\n",
            "2025-08-09 19:47:31.716410: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 575.64.5\n",
            "2025-08-09 19:47:31.716452: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 575.64.5\n",
            "2025-08-09 19:47:31.716460: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 575.64.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_cybersecurity_attacks.csv\n",
            "  üíæ All models saved to saved_models/cybersecurity_attacks\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 2/4: ../datasets/2good_reqff.csv ---\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step\n",
            "  ‚úÖ Generated scores using supervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 5217\n",
            "Level 1 Threat (Medium)       291\n",
            "Level 2 Threat (High)         231\n",
            "Level 3 Threat (Critical)      58\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_goodbad_requests.csv\n",
            "  üíæ All models saved to saved_models/goodbad_requests\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 3/4: ../datasets/wls_day-02.csv ---\n",
            "  üìÑ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  ‚úÖ Final unsupervised dataset: 2604 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 2343\n",
            "Level 1 Threat (Medium)       130\n",
            "Level 2 Threat (High)         104\n",
            "Level 3 Threat (Critical)      27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_wls_events.csv\n",
            "  üíæ All models saved to saved_models/wls_events\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 4/4: ../datasets/netflow_day-02.csv ---\n",
            "  üìÑ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_netflow.csv\n",
            "  üíæ All models saved to saved_models/netflow_analysis\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "üéâüéâ Pipeline finished successfully! üéâüéâ\n",
            "üìÅ All models saved in 'saved_models' directory\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import os\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "\n",
        "def hex_to_int(val):\n",
        "    \"\"\"Safely convert hex or string numbers to integers.\"\"\"\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def load_and_preprocess_unsupervised(csv_path, feature_cols, max_rows=10000):\n",
        "    \"\"\"Loads and preprocesses a single file for unsupervised learning.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  üìÑ Loaded {len(df)} rows from {csv_path}\")\n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  üìâ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "\n",
        "        print(f\"  ‚úÖ Final unsupervised dataset: {len(df)} rows\")\n",
        "        return df, df[feature_cols].values\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error in unsupervised loading for {csv_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def train_autoencoder(X_train_scaled):\n",
        "    \"\"\"Trains a standard autoencoder model.\"\"\"\n",
        "    print(\"  ü§ñ Training autoencoder...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(max(8, int(input_dim * 0.5)), activation=\"relu\")(encoder)\n",
        "    decoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(encoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_train_scaled, X_train_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"  ‚úÖ Autoencoder training complete.\")\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def save_models(dataset_name, autoencoder, scaler, feature_names):\n",
        "    \"\"\"Save autoencoder and scaler for a specific dataset\"\"\"\n",
        "    model_dir = f\"saved_models/{dataset_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save autoencoder (Keras model)\n",
        "    autoencoder.save(f\"{model_dir}/autoencoder.keras\")\n",
        "    \n",
        "    # Save scaler\n",
        "    joblib.dump(scaler, f\"{model_dir}/scaler.pkl\")\n",
        "    \n",
        "    # Save feature names\n",
        "    with open(f\"{model_dir}/feature_names.pkl\", 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "    \n",
        "    print(f\"  üíæ All models saved to {model_dir}\")\n",
        "\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "csv_list = [\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/cybersecurity_attacks.csv\",\n",
        "        \"features\": [\"Packet Length\", \"Source Port\", \"Destination Port\"],\n",
        "        \"out_csv\": \"classified_cybersecurity_attacks.csv\",\n",
        "        \"model_name\": \"cybersecurity_attacks\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"supervised\", # This dataset uses the special supervised method\n",
        "        \"good_path\": \"../datasets/2good_reqff.csv\",\n",
        "        \"bad_path\": \"../datasets/2bad_reqff.csv\",\n",
        "        \"features\": [\"path_length\", \"body_length\", \"badwords_count\"],\n",
        "        \"out_csv\": \"classified_goodbad_requests.csv\",\n",
        "        \"model_name\": \"goodbad_requests\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/wls_day-02.csv\",\n",
        "        \"features\": [\"ProcessID\", \"ParentProcessID\", \"EventID\"],\n",
        "        \"out_csv\": \"classified_wls_events.csv\",\n",
        "        \"model_name\": \"wls_events\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/netflow_day-02.csv\",\n",
        "        \"features\": [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"],\n",
        "        \"out_csv\": \"classified_netflow.csv\",\n",
        "        \"model_name\": \"netflow_analysis\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create main models directory\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# === EXECUTION PIPELINE ===\n",
        "print(\"üöÄ Starting Hybrid Anomaly Classification Pipeline\\n\")\n",
        "\n",
        "for i, config in enumerate(csv_list, 1):\n",
        "    print(f\"--- [‚öôÔ∏è] Processing Dataset {i}/{len(csv_list)}: {config.get('path') or config.get('good_path')} ---\")\n",
        "\n",
        "    df, final_score_col = None, None\n",
        "    autoencoder, scaler = None, None\n",
        "\n",
        "    if config[\"type\"] == \"supervised\":\n",
        "        try:\n",
        "            good_df = pd.read_csv(config['good_path'])\n",
        "            bad_df = pd.read_csv(config['bad_path'])\n",
        "            features = config['features']\n",
        "\n",
        "            X_train_good = good_df[features].values\n",
        "\n",
        "            df = pd.concat([good_df, bad_df], ignore_index=True)\n",
        "            X_all = df[features].values\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_good_scaled = scaler.fit_transform(X_train_good)\n",
        "            X_all_scaled = scaler.transform(X_all)\n",
        "\n",
        "            autoencoder = train_autoencoder(X_train_good_scaled)\n",
        "            reconstructions = autoencoder.predict(X_all_scaled)\n",
        "            df['Final_Score'] = np.mean(np.abs(X_all_scaled - reconstructions), axis=1)\n",
        "            print(\"  ‚úÖ Generated scores using supervised method.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error in supervised loading: {e}\")\n",
        "            continue\n",
        "\n",
        "    elif config[\"type\"] == \"unsupervised\":\n",
        "        df, X_features = load_and_preprocess_unsupervised(config['path'], config['features'])\n",
        "        if df is None:\n",
        "            print(f\"--- ‚è≠Ô∏è  Skipping {config['path']} ---\\n\")\n",
        "            continue\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        X_scaled = scaler.fit_transform(X_features)\n",
        "        autoencoder = train_autoencoder(X_scaled)\n",
        "        predictions = autoencoder.predict(X_scaled)\n",
        "        df['Final_Score'] = np.mean(np.abs(X_scaled - predictions), axis=1)\n",
        "        print(\"  ‚úÖ Generated scores using unsupervised method.\")\n",
        "\n",
        "    # --- Classify into threat levels based on the Final_Score ---\n",
        "    if df is not None:\n",
        "        print(\"  üîé Classifying threats into levels...\")\n",
        "        level_3_threshold = df['Final_Score'].quantile(0.99)\n",
        "        level_2_threshold = df['Final_Score'].quantile(0.95)\n",
        "        level_1_threshold = df['Final_Score'].quantile(0.90)\n",
        "\n",
        "        conditions = [\n",
        "            df['Final_Score'] > level_3_threshold,\n",
        "            df['Final_Score'] > level_2_threshold,\n",
        "            df['Final_Score'] > level_1_threshold\n",
        "        ]\n",
        "        levels = ['Level 3 Threat (Critical)', 'Level 2 Threat (High)', 'Level 1 Threat (Medium)']\n",
        "        df['Threat_Level'] = np.select(conditions, levels, default='Not a Threat')\n",
        "\n",
        "        print(\"\\n  --- Threat Level Distribution ---\")\n",
        "        print(df['Threat_Level'].value_counts())\n",
        "        df.to_csv(config['out_csv'], index=False)\n",
        "        print(f\"\\n  üíæ Results saved to {config['out_csv']}\")\n",
        "        \n",
        "        # Save the trained models\n",
        "        if autoencoder is not None and scaler is not None:\n",
        "            save_models(config['model_name'], autoencoder, scaler, config['features'])\n",
        "        \n",
        "        print(f\"\\n--- ‚úÖ Completed processing ---\\n\")\n",
        "\n",
        "print(\"üéâüéâ Pipeline finished successfully! üéâüéâ\")\n",
        "print(\"üìÅ All models saved in 'saved_models' directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Enhanced Hybrid Anomaly Classification Pipeline with ML Models\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 1/4: ../datasets/cybersecurity_attacks.csv ---\n",
            "  üìÑ Loaded 40000 rows from ../datasets/cybersecurity_attacks.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.0008, Max: 0.0845, Mean: 0.0200\n",
            "  üå≤ Training Random Forest for SHAP explanations...\n",
            "  üîç Creating SHAP explainer...\n",
            "  ‚úÖ RandomForest and SHAP explainer ready.\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_cybersecurity_attacks.csv\n",
            "    ‚úÖ Autoencoder saved\n",
            "    ‚úÖ Scaler saved\n",
            "    ‚úÖ RandomForest saved\n",
            "    ‚úÖ SHAP explainer saved\n",
            "    ‚úÖ Feature names saved\n",
            "  üíæ All models saved to saved_models/cybersecurity_attacks\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 2/4: ../datasets/2good_reqff.csv ---\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "  üå≤ Training Random Forest for SHAP explanations...\n",
            "  üîç Creating SHAP explainer...\n",
            "  ‚úÖ RandomForest and SHAP explainer ready.\n",
            "  ‚úÖ Generated scores using supervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 5218\n",
            "Level 1 Threat (Medium)       289\n",
            "Level 2 Threat (High)         232\n",
            "Level 3 Threat (Critical)      58\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_goodbad_requests.csv\n",
            "    ‚úÖ Autoencoder saved\n",
            "    ‚úÖ Scaler saved\n",
            "    ‚úÖ RandomForest saved\n",
            "    ‚úÖ SHAP explainer saved\n",
            "    ‚úÖ Feature names saved\n",
            "  üíæ All models saved to saved_models/goodbad_requests\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 3/4: ../datasets/wls_day-02.csv ---\n",
            "  üìÑ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  ‚úÖ Final unsupervised dataset: 2604 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.0000, Max: 0.3194, Mean: 0.0058\n",
            "  üå≤ Training Random Forest for SHAP explanations...\n",
            "  üîç Creating SHAP explainer...\n",
            "  ‚úÖ RandomForest and SHAP explainer ready.\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 2343\n",
            "Level 1 Threat (Medium)       130\n",
            "Level 2 Threat (High)         104\n",
            "Level 3 Threat (Critical)      27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_wls_events.csv\n",
            "    ‚úÖ Autoencoder saved\n",
            "    ‚úÖ Scaler saved\n",
            "    ‚úÖ RandomForest saved\n",
            "    ‚úÖ SHAP explainer saved\n",
            "    ‚úÖ Feature names saved\n",
            "  üíæ All models saved to saved_models/wls_events\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "--- [‚öôÔ∏è] Processing Dataset 4/4: ../datasets/netflow_day-02.csv ---\n",
            "  üìÑ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  üìâ Sampled down to 10000 rows\n",
            "  ‚úÖ Final unsupervised dataset: 10000 rows\n",
            "  ü§ñ Training autoencoder...\n",
            "  ‚úÖ Autoencoder training complete.\n",
            "  üìä Computing anomaly scores...\n",
            "  üìà Anomaly scores - Min: 0.0000, Max: 0.7256, Mean: 0.0022\n",
            "  üå≤ Training Random Forest for SHAP explanations...\n",
            "  üîç Creating SHAP explainer...\n",
            "  ‚úÖ RandomForest and SHAP explainer ready.\n",
            "  ‚úÖ Generated scores using unsupervised method.\n",
            "  üîé Classifying threats into levels...\n",
            "\n",
            "  --- Threat Level Distribution ---\n",
            "Threat_Level\n",
            "Not a Threat                 9000\n",
            "Level 1 Threat (Medium)       500\n",
            "Level 2 Threat (High)         400\n",
            "Level 3 Threat (Critical)     100\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  üíæ Results saved to classified_netflow.csv\n",
            "    ‚úÖ Autoencoder saved\n",
            "    ‚úÖ Scaler saved\n",
            "    ‚úÖ RandomForest saved\n",
            "    ‚úÖ SHAP explainer saved\n",
            "    ‚úÖ Feature names saved\n",
            "  üíæ All models saved to saved_models/netflow_analysis\n",
            "\n",
            "--- ‚úÖ Completed processing ---\n",
            "\n",
            "üéâüéâ Pipeline finished successfully! üéâüéâ\n",
            "üìÅ All models saved in 'saved_models' directory\n",
            "\n",
            "üìÇ Final saved_models directory structure:\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import shap\n",
        "import os\n",
        "import joblib\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def hex_to_int(val):\n",
        "    \"\"\"Safely convert hex or string numbers to integers.\"\"\"\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def load_and_preprocess_unsupervised(csv_path, feature_cols, max_rows=10000):\n",
        "    \"\"\"Loads and preprocesses a single file for unsupervised learning.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  üìÑ Loaded {len(df)} rows from {csv_path}\")\n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  üìâ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "        df.dropna(subset=feature_cols, inplace=True)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "\n",
        "        print(f\"  ‚úÖ Final unsupervised dataset: {len(df)} rows\")\n",
        "        return df, df[feature_cols].values\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error in unsupervised loading for {csv_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def train_autoencoder(X_train_scaled):\n",
        "    \"\"\"Trains a standard autoencoder model.\"\"\"\n",
        "    print(\"  ü§ñ Training autoencoder...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(max(8, int(input_dim * 0.5)), activation=\"relu\")(encoder)\n",
        "    decoder = Dense(max(16, int(input_dim * 0.75)), activation=\"relu\")(encoder)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "    autoencoder.fit(X_train_scaled, X_train_scaled, epochs=20, batch_size=32, shuffle=True, verbose=0)\n",
        "    print(\"  ‚úÖ Autoencoder training complete.\")\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def compute_anomaly_scores(autoencoder, X_scaled):\n",
        "    \"\"\"Compute anomaly scores using autoencoder.\"\"\"\n",
        "    print(\"  üìä Computing anomaly scores...\")\n",
        "    predictions = autoencoder.predict(X_scaled, verbose=0)\n",
        "    scores = np.mean(np.abs(X_scaled - predictions), axis=1)\n",
        "    print(f\"  üìà Anomaly scores - Min: {scores.min():.4f}, Max: {scores.max():.4f}, Mean: {scores.mean():.4f}\")\n",
        "    return scores\n",
        "\n",
        "\n",
        "def train_random_forest_and_shap(X_scaled, anomaly_scores, feature_names):\n",
        "    \"\"\"Train RandomForest and create SHAP explainer.\"\"\"\n",
        "    print(\"  üå≤ Training Random Forest for SHAP explanations...\")\n",
        "    \n",
        "    # Train Random Forest on anomaly scores\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_scaled, anomaly_scores)\n",
        "    \n",
        "    # Create SHAP explainer\n",
        "    print(\"  üîç Creating SHAP explainer...\")\n",
        "    # Use a sample of data for faster explainer creation\n",
        "    sample_size = min(200, len(X_scaled))\n",
        "    sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
        "    sample_data = X_scaled[sample_indices]\n",
        "    \n",
        "    explainer = shap.TreeExplainer(rf_model, data=sample_data, feature_perturbation=\"interventional\")\n",
        "    \n",
        "    print(\"  ‚úÖ RandomForest and SHAP explainer ready.\")\n",
        "    return rf_model, explainer\n",
        "\n",
        "\n",
        "def save_models(dataset_name, autoencoder, scaler, rf_model, shap_explainer, feature_names):\n",
        "    \"\"\"Save all models for a specific dataset\"\"\"\n",
        "    model_dir = f\"saved_models/{dataset_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save autoencoder (Keras model)\n",
        "    autoencoder.save(f\"{model_dir}/autoencoder.keras\")\n",
        "    print(f\"    ‚úÖ Autoencoder saved\")\n",
        "    \n",
        "    # Save scaler\n",
        "    joblib.dump(scaler, f\"{model_dir}/scaler.pkl\")\n",
        "    print(f\"    ‚úÖ Scaler saved\")\n",
        "    \n",
        "    # Save Random Forest model\n",
        "    joblib.dump(rf_model, f\"{model_dir}/random_forest.pkl\")\n",
        "    print(f\"    ‚úÖ RandomForest saved\")\n",
        "    \n",
        "    # Save SHAP explainer\n",
        "    with open(f\"{model_dir}/shap_explainer.pkl\", 'wb') as f:\n",
        "        pickle.dump(shap_explainer, f)\n",
        "    print(f\"    ‚úÖ SHAP explainer saved\")\n",
        "    \n",
        "    # Save feature names\n",
        "    with open(f\"{model_dir}/feature_names.pkl\", 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "    print(f\"    ‚úÖ Feature names saved\")\n",
        "    \n",
        "    print(f\"  üíæ All models saved to {model_dir}\")\n",
        "\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "csv_list = [\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/cybersecurity_attacks.csv\",\n",
        "        \"features\": [\"Packet Length\", \"Source Port\", \"Destination Port\"],\n",
        "        \"out_csv\": \"classified_cybersecurity_attacks.csv\",\n",
        "        \"model_name\": \"cybersecurity_attacks\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"supervised\", # This dataset uses the special supervised method\n",
        "        \"good_path\": \"../datasets/2good_reqff.csv\",\n",
        "        \"bad_path\": \"../datasets/2bad_reqff.csv\",\n",
        "        \"features\": [\"path_length\", \"body_length\", \"badwords_count\"],\n",
        "        \"out_csv\": \"classified_goodbad_requests.csv\",\n",
        "        \"model_name\": \"goodbad_requests\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/wls_day-02.csv\",\n",
        "        \"features\": [\"ProcessID\", \"ParentProcessID\", \"EventID\"],\n",
        "        \"out_csv\": \"classified_wls_events.csv\",\n",
        "        \"model_name\": \"wls_events\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"unsupervised\",\n",
        "        \"path\": \"../datasets/netflow_day-02.csv\",\n",
        "        \"features\": [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"],\n",
        "        \"out_csv\": \"classified_netflow.csv\",\n",
        "        \"model_name\": \"netflow_analysis\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create main models directory\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# === EXECUTION PIPELINE ===\n",
        "print(\"üöÄ Starting Enhanced Hybrid Anomaly Classification Pipeline with ML Models\\n\")\n",
        "\n",
        "for i, config in enumerate(csv_list, 1):\n",
        "    print(f\"--- [‚öôÔ∏è] Processing Dataset {i}/{len(csv_list)}: {config.get('path') or config.get('good_path')} ---\")\n",
        "\n",
        "    df, final_score_col = None, None\n",
        "    autoencoder, scaler, rf_model, shap_explainer = None, None, None, None\n",
        "\n",
        "    if config[\"type\"] == \"supervised\":\n",
        "        try:\n",
        "            good_df = pd.read_csv(config['good_path'])\n",
        "            bad_df = pd.read_csv(config['bad_path'])\n",
        "            features = config['features']\n",
        "\n",
        "            X_train_good = good_df[features].values\n",
        "\n",
        "            df = pd.concat([good_df, bad_df], ignore_index=True)\n",
        "            X_all = df[features].values\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_good_scaled = scaler.fit_transform(X_train_good)\n",
        "            X_all_scaled = scaler.transform(X_all)\n",
        "\n",
        "            autoencoder = train_autoencoder(X_train_good_scaled)\n",
        "            reconstructions = autoencoder.predict(X_all_scaled, verbose=0)\n",
        "            anomaly_scores = np.mean(np.abs(X_all_scaled - reconstructions), axis=1)\n",
        "            df['Final_Score'] = anomaly_scores\n",
        "            \n",
        "            # Train Random Forest and create SHAP explainer\n",
        "            rf_model, shap_explainer = train_random_forest_and_shap(X_all_scaled, anomaly_scores, features)\n",
        "            \n",
        "            print(\"  ‚úÖ Generated scores using supervised method.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error in supervised loading: {e}\")\n",
        "            continue\n",
        "\n",
        "    elif config[\"type\"] == \"unsupervised\":\n",
        "        df, X_features = load_and_preprocess_unsupervised(config['path'], config['features'])\n",
        "        if df is None:\n",
        "            print(f\"--- ‚è≠Ô∏è  Skipping {config['path']} ---\\n\")\n",
        "            continue\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        X_scaled = scaler.fit_transform(X_features)\n",
        "        autoencoder = train_autoencoder(X_scaled)\n",
        "        \n",
        "        # Compute anomaly scores\n",
        "        anomaly_scores = compute_anomaly_scores(autoencoder, X_scaled)\n",
        "        df['Final_Score'] = anomaly_scores\n",
        "        \n",
        "        # Train Random Forest and create SHAP explainer\n",
        "        rf_model, shap_explainer = train_random_forest_and_shap(X_scaled, anomaly_scores, config['features'])\n",
        "        \n",
        "        print(\"  ‚úÖ Generated scores using unsupervised method.\")\n",
        "\n",
        "    # --- Classify into threat levels based on the Final_Score ---\n",
        "    if df is not None:\n",
        "        print(\"  üîé Classifying threats into levels...\")\n",
        "        level_3_threshold = df['Final_Score'].quantile(0.99)\n",
        "        level_2_threshold = df['Final_Score'].quantile(0.95)\n",
        "        level_1_threshold = df['Final_Score'].quantile(0.90)\n",
        "\n",
        "        conditions = [\n",
        "            df['Final_Score'] > level_3_threshold,\n",
        "            df['Final_Score'] > level_2_threshold,\n",
        "            df['Final_Score'] > level_1_threshold\n",
        "        ]\n",
        "        levels = ['Level 3 Threat (Critical)', 'Level 2 Threat (High)', 'Level 1 Threat (Medium)']\n",
        "        df['Threat_Level'] = np.select(conditions, levels, default='Not a Threat')\n",
        "\n",
        "        print(\"\\n  --- Threat Level Distribution ---\")\n",
        "        print(df['Threat_Level'].value_counts())\n",
        "        df.to_csv(config['out_csv'], index=False)\n",
        "        print(f\"\\n  üíæ Results saved to {config['out_csv']}\")\n",
        "        \n",
        "        # Save all trained models\n",
        "        if all(model is not None for model in [autoencoder, scaler, rf_model, shap_explainer]):\n",
        "            save_models(config['model_name'], autoencoder, scaler, rf_model, shap_explainer, config['features'])\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  Some models are None, skipping save...\")\n",
        "        \n",
        "        print(f\"\\n--- ‚úÖ Completed processing ---\\n\")\n",
        "\n",
        "print(\"üéâüéâ Pipeline finished successfully! üéâüéâ\")\n",
        "print(\"üìÅ All models saved in 'saved_models' directory\")\n",
        "\n",
        "# Print final directory structure\n",
        "print(\"\\nüìÇ Final saved_models directory structure:\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
